"""
Daliaæ•°æ®é›†å¿«é€Ÿä¿®å¤è„šæœ¬ - åŸºäºè°ƒè¯•ç»“æœæ›´æ–°
ç›´æ¥è§£å†³pickleåŠ è½½å’ŒDataFrameåˆ›å»ºé—®é¢˜
"""

import os
import pickle
import pandas as pd
import numpy as np


def quick_fix_dalia_dataset(data_dir=r"F:\ppg+dalia\data\PPG_FieldStudy"):
    """å¿«é€Ÿä¿®å¤Daliaæ•°æ®é›†é—®é¢˜"""

    print("å¼€å§‹å¿«é€Ÿä¿®å¤Daliaæ•°æ®é›†...")

    # åˆ›å»ºprocessedç›®å½•
    processed_dir = os.path.join(data_dir, "processed")
    os.makedirs(processed_dir, exist_ok=True)

    # æ•°æ®é›†åˆ’åˆ†
    splits = {
        'train': list(range(1, 13)),  # S1-S12
        'val': [13, 14],              # S13-S14
        'test': [15]                  # S15
    }

    def safe_load_pickle(file_path):
        """å®‰å…¨åŠ è½½pickleæ–‡ä»¶ - åŸºäºè°ƒè¯•ç»“æœ"""
        if not os.path.exists(file_path):
            return None

        # æ ¹æ®è°ƒè¯•ç»“æœï¼Œæ–¹æ³•2 (latin-1) æœ€æœ‰æ•ˆ
        methods = [
            lambda f: pickle.load(f, encoding='latin-1'),  # ä¼˜å…ˆä½¿ç”¨è¿™ä¸ª
            lambda f: pickle.load(f),
            lambda f: pickle.load(f, encoding='bytes')
        ]

        for i, method in enumerate(methods):
            try:
                with open(file_path, 'rb') as f:
                    data = method(f)
                    if i == 0:
                        print(f"    âœ“ ä½¿ç”¨latin-1ç¼–ç åŠ è½½æˆåŠŸ")
                    return data
            except Exception as e:
                if i == 0:
                    print(f"    âœ— latin-1ç¼–ç å¤±è´¥: {e}")
                continue
        return None

    def extract_subject_info(subject_data):
        """æå–å—è¯•è€…ä¿¡æ¯ - ä¿®å¤æ•°ç»„æ¯”è¾ƒé—®é¢˜"""
        if not subject_data:
            return {
                'hr_labels': [70.0] * 100,
                'avg_hr': 70.0,
                'age': 30,
                'gender': 'unknown',
                'weight': 70,
                'height': 170
            }

        # æå–å¿ƒç‡æ ‡ç­¾
        hr_labels = subject_data.get('label', [])

        if hr_labels is not None and len(hr_labels) > 0:
            # è½¬æ¢ä¸ºnumpyæ•°ç»„ä»¥ä¾¿å¤„ç†
            hr_array = np.array(hr_labels)

            # è¿‡æ»¤æœ‰æ•ˆå¿ƒç‡å€¼ (30-200 bpm) - ä¿®å¤æ•°ç»„æ¯”è¾ƒé—®é¢˜
            # ä½¿ç”¨numpyçš„å‘é‡åŒ–æ“ä½œ
            valid_mask = (
                np.isfinite(hr_array) &  # ä¸æ˜¯NaNæˆ–inf
                (hr_array >= 30) &       # æœ€å°å€¼
                (hr_array <= 200)        # æœ€å¤§å€¼
            )

            valid_labels = hr_array[valid_mask]

            if len(valid_labels) > 0:
                avg_hr = float(np.mean(valid_labels))
                # è½¬æ¢å›Pythonåˆ—è¡¨
                hr_labels_list = [float(hr) for hr in valid_labels]
            else:
                avg_hr = 70.0
                hr_labels_list = [70.0] * 100
        else:
            avg_hr = 70.0
            hr_labels_list = [70.0] * 100

        # æå–questionnaireä¿¡æ¯
        questionnaire = subject_data.get('questionnaire', {})

        # å®‰å…¨æå–å­—æ®µ
        def safe_extract(data, key, default, convert_func=None):
            """å®‰å…¨æå–å­—æ®µå€¼"""
            try:
                value = data.get(key, default)
                if value is None or value == '':
                    return default
                if convert_func:
                    return convert_func(value)
                return value
            except (ValueError, TypeError, AttributeError):
                return default

        # ä»è°ƒè¯•è¾“å‡ºå¯çŸ¥å­—æ®µåä¸º: 'AGE', 'Gender', 'WEIGHT', 'HEIGHT', 'SKIN', 'SPORT'
        age = safe_extract(questionnaire, 'AGE', 30, lambda x: int(float(x)))
        gender = safe_extract(questionnaire, 'Gender', 'unknown', str)
        weight = safe_extract(questionnaire, 'WEIGHT', 70.0, float)
        height = safe_extract(questionnaire, 'HEIGHT', 170.0, float)

        # å¤„ç†å¯èƒ½çš„bytesç±»å‹
        if isinstance(gender, bytes):
            gender = gender.decode('utf-8', errors='ignore')

        return {
            'hr_labels': hr_labels_list,
            'avg_hr': avg_hr,
            'age': age,
            'gender': str(gender),
            'weight': weight,
            'height': height
        }

    # ä¸ºæ¯ä¸ªåˆ†å‰²åˆ›å»ºCSVæ–‡ä»¶
    all_data = {}  # å­˜å‚¨æ‰€æœ‰å—è¯•è€…æ•°æ®

    print("\nåŠ è½½æ‰€æœ‰å—è¯•è€…æ•°æ®...")
    for subject_id in range(1, 16):
        pkl_file = os.path.join(data_dir, f"S{subject_id}", f"S{subject_id}.pkl")
        print(f"åŠ è½½ S{subject_id}...")

        # åŠ è½½æ•°æ®
        subject_data = safe_load_pickle(pkl_file)

        # æå–ä¿¡æ¯
        info = extract_subject_info(subject_data)
        all_data[subject_id] = info

        print(f"  å¿ƒç‡: {info['avg_hr']:.1f} bpm (å…±{len(info['hr_labels'])}ä¸ªæ ‡ç­¾)")
        print(f"  åŸºæœ¬ä¿¡æ¯: {info['age']}å², {info['gender']}, {info['weight']}kg, {info['height']}cm")

    print("\nåˆ›å»ºCSVæ–‡ä»¶...")
    for split_name, subject_ids in splits.items():
        print(f"å¤„ç† {split_name} åˆ†å‰²...")

        data = []

        for subject_id in subject_ids:
            info = all_data.get(subject_id, {})

            # ä¸ºæ¯ä¸ªå—è¯•è€…åˆ›å»ºå¤šè¡Œæ•°æ®ï¼ˆæ¯ä¸ªå¿ƒç‡æ ‡ç­¾ä¸€è¡Œï¼‰
            hr_labels = info.get('hr_labels', [70.0])

            # é™åˆ¶æ®µæ•°é¿å…å†…å­˜é—®é¢˜
            max_segments = min(len(hr_labels), 200)

            for i in range(max_segments):
                hr_value = hr_labels[i] if i < len(hr_labels) else info.get('avg_hr', 70.0)

                data.append({
                    'subject_ID': subject_id,
                    'segment_idx': i,
                    'hr': float(hr_value),
                    'age': info.get('age', 30),
                    'gender': info.get('gender', 'unknown'),
                    'weight': info.get('weight', 70.0),
                    'height': info.get('height', 170.0),
                    'num_segments': len(hr_labels)
                })

        # ä¿å­˜CSV
        df = pd.DataFrame(data)
        csv_path = os.path.join(processed_dir, f"{split_name}.csv")
        df.to_csv(csv_path, index=False)
        print(f"âœ“ ä¿å­˜ {csv_path}: {len(df)} è¡Œæ•°æ®")

        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        unique_subjects = df['subject_ID'].nunique()
        avg_hr_range = f"{df['hr'].min():.1f}-{df['hr'].max():.1f}"
        print(f"  {unique_subjects} ä¸ªå—è¯•è€…, å¿ƒç‡èŒƒå›´: {avg_hr_range} bpm")

    print("\nå¿«é€Ÿä¿®å¤å®Œæˆï¼")
    return True


def test_fixed_data(data_dir=r"F:\ppg+dalia\data\PPG_FieldStudy"):
    """æµ‹è¯•ä¿®å¤åçš„æ•°æ®"""
    processed_dir = os.path.join(data_dir, "processed")

    print("\n" + "="*50)
    print("æµ‹è¯•ä¿®å¤åçš„æ•°æ®...")

    total_samples = 0

    for split in ['train', 'val', 'test']:
        csv_file = os.path.join(processed_dir, f"{split}.csv")

        if os.path.exists(csv_file):
            df = pd.read_csv(csv_file)
            total_samples += len(df)

            print(f"\n{split}.csv:")
            print(f"  è¡Œæ•°: {len(df)}")
            print(f"  åˆ—: {list(df.columns)}")
            print(f"  å—è¯•è€…: {df['subject_ID'].nunique()} ä¸ª")
            print(f"  å¿ƒç‡èŒƒå›´: {df['hr'].min():.1f} - {df['hr'].max():.1f} bpm")

            # æ£€æŸ¥å¿…è¦çš„åˆ—
            required_cols = ['subject_ID', 'hr', 'segment_idx']
            missing_cols = [col for col in required_cols if col not in df.columns]

            if missing_cols:
                print(f"  âš ï¸  ç¼ºå°‘åˆ—: {missing_cols}")
            else:
                print(f"  âœ… åŒ…å«æ‰€æœ‰å¿…è¦åˆ—")

            # æ˜¾ç¤ºå—è¯•è€…ä¿¡æ¯
            subjects = sorted(df['subject_ID'].unique())
            print(f"  å—è¯•è€…ID: S{min(subjects)}-S{max(subjects)}")

        else:
            print(f"\n{split}.csv: âŒ æ–‡ä»¶ä¸å­˜åœ¨")

    print(f"\næ€»æ•°æ®æ ·æœ¬: {total_samples}")
    print("="*50)


def create_simple_test_data(data_dir=r"F:\ppg+dalia\data\PPG_FieldStudy"):
    """åˆ›å»ºç®€åŒ–æµ‹è¯•æ•°æ®ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰"""
    processed_dir = os.path.join(data_dir, "processed")
    os.makedirs(processed_dir, exist_ok=True)

    print("åˆ›å»ºç®€åŒ–æµ‹è¯•æ•°æ®...")

    # ç®€åŒ–çš„æ•°æ®é›†åˆ’åˆ†
    splits = {
        'train': list(range(1, 13)),  # S1-S12
        'val': [13, 14],              # S13-S14
        'test': [15]                  # S15
    }

    for split_name, subject_ids in splits.items():
        data = []

        for subject_id in subject_ids:
            # ä¸ºæ¯ä¸ªå—è¯•è€…åˆ›å»º50ä¸ªæ•°æ®æ®µ
            for i in range(50):
                hr = 60 + subject_id + np.random.normal(0, 5)  # åŸºäºå—è¯•è€…IDçš„å¿ƒç‡
                hr = max(50, min(120, hr))  # é™åˆ¶åœ¨åˆç†èŒƒå›´

                data.append({
                    'subject_ID': subject_id,
                    'segment_idx': i,
                    'hr': float(hr),
                    'age': 25 + subject_id,
                    'gender': 'M' if subject_id % 2 == 1 else 'F',
                    'weight': 60 + subject_id * 2,
                    'height': 160 + subject_id,
                    'num_segments': 50
                })

        df = pd.DataFrame(data)
        csv_path = os.path.join(processed_dir, f"{split_name}.csv")
        df.to_csv(csv_path, index=False)
        print(f"âœ“ åˆ›å»º {csv_path}: {len(df)} è¡Œ")


if __name__ == "__main__":
    # è®¾ç½®æ•°æ®ç›®å½•
    data_dir = r"F:\ppg+dalia\data\PPG_FieldStudy"

    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(data_dir):
        print(f"é”™è¯¯: æ•°æ®ç›®å½•ä¸å­˜åœ¨ {data_dir}")
        print("è¯·ä¿®æ”¹data_dirå˜é‡ä¸ºæ­£ç¡®çš„è·¯å¾„")
        exit(1)

    try:
        # æ‰§è¡Œå¿«é€Ÿä¿®å¤
        if quick_fix_dalia_dataset(data_dir):
            # æµ‹è¯•ä¿®å¤ç»“æœ
            test_fixed_data(data_dir)

            print("\n" + "ğŸ‰" * 20)
            print("âœ… ä¿®å¤å®Œæˆï¼ç°åœ¨å¯ä»¥è¿è¡Œä¸»ç¨‹åº:")
            print("   python main.py")
            print("ğŸ‰" * 20)
        else:
            print("âŒ ä¿®å¤å¤±è´¥")

    except Exception as e:
        print(f"âŒ ä¿®å¤è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        print("\nå°è¯•åˆ›å»ºç®€åŒ–æµ‹è¯•æ•°æ®...")
        create_simple_test_data(data_dir)
        print("âœ… ç®€åŒ–æ•°æ®åˆ›å»ºå®Œæˆï¼Œå¯ä»¥ç”¨äºæµ‹è¯•è®­ç»ƒæµç¨‹")